from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from pathlib import Path
import fitz
import os
import gradio as gr
import requests
import sseclient
import json

# è¿œç¨‹ Qwen API æµå¼å“åº”å°è£…
def stream_qwen_response(prompt):
    api_key = os.getenv("SILICON_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® SILICON_API_KEY ç¯å¢ƒå˜é‡")

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Accept": "text/event-stream",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "Qwen/Qwen3-8B",
        "stream": True,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    }

    with requests.post("https://api.siliconflow.cn/v1/chat/completions", headers=headers, json=payload, stream=True) as response:
        if response.status_code != 200:
            raise RuntimeError(f"è¯·æ±‚å¤±è´¥ï¼š{response.status_code} {response.text}")

        client = sseclient.SSEClient(response)
        for event in client.events():
            if event.data == "[DONE]":
                break
            try:
                delta = json.loads(event.data)
                content = delta.get("choices", [{}])[0].get("delta", {}).get("content", "")
                yield content
            except Exception as e:
                yield f"[æµå¼è§£æé”™è¯¯] {e}"

# åŠ è½½ PDF æ–‡ä»¶å¤¹
def load_pdfs_from_folder(folder_path):
    documents = []
    for file in Path(folder_path).rglob("*.pdf"):
        try:
            text = ""
            with fitz.open(str(file)) as doc:
                for page in doc:
                    text += page.get_text()
            if text.strip():
                documents.append(Document(page_content=text, metadata={"source": str(file)}))
        except Exception as e:
            print(f"è¯»å–å¤±è´¥: {file} åŸå› : {e}")
    return documents

# æ„å»ºä»…æ£€ç´¢çš„å‘é‡æ£€ç´¢å™¨
def build_retriever_from_docs(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)
    if not chunks:
        raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“")

    embedder = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh",
        model_kwargs={"device": "cpu"}
    )
    vectordb = FAISS.from_documents(chunks, embedder)
    return vectordb.as_retriever(search_kwargs={"k": 20})

# æ„å»ºæœç´¢å‡½æ•°ï¼ˆå¯¹æ£€ç´¢ç»“æœè¿›è¡Œæµå¼æ€»ç»“ï¼‰
def stream_search_docs(query, retriever):
    results = retriever.get_relevant_documents(query)
    if not results:
        yield "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        return

    filtered = [doc for doc in results if query.lower() in doc.page_content.lower()]
    if not filtered:
        yield "æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„å†…å®¹ï¼Œè¯·å°è¯•æ¢ä¸ªè¡¨è¾¾"
        return

    combined_text = "\n".join(doc.page_content[:1000] for doc in filtered[:3])
    try:
        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆç®€æ´æ¸…æ™°çš„æ—…æ¸¸æ¨èæ‘˜è¦ï¼š\n\n{combined_text}\n\næ‘˜è¦ï¼š"
        for chunk in stream_qwen_response(prompt):
            yield chunk
    except Exception as e:
        yield f"å¤§æ¨¡å‹æ€»ç»“å¤±è´¥ï¼Œæ”¹ä¸ºæ˜¾ç¤ºåŸæ–‡ï¼š\n\n{combined_text}"

# åŠ è½½ç¯å¢ƒå˜é‡
def load_env(filepath):
    env = {}
    if os.path.exists(filepath):
        with open(filepath, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" in line:
                    k, v = line.split("=", 1)
                    env[k.strip()] = v.strip()
    return env

if __name__ == "__main__":
    env_path = Path(__file__).resolve().parent.parent / "API.env"
    env_vars = load_env(env_path)
    os.environ.update(env_vars)

    dataset_dir = Path(__file__).resolve().parent.parent / "dataset"
    rag_docs = load_pdfs_from_folder(dataset_dir)
    if not rag_docs:
        raise RuntimeError("PDF æ–‡ä»¶å¤¹ä¸­æœªæˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ dataset è·¯å¾„ä¸ PDF å†…å®¹")
    retriever = build_retriever_from_docs(rag_docs)

    with gr.Blocks() as demo:
        with gr.Tab("ğŸ“š æ–‡æ¡£é—®ç­”åŠ©æ‰‹"):
            gr.Markdown("### è¾“å…¥å…³é”®è¯ï¼ˆå¦‚åŸå¸‚åï¼‰ï¼Œä»PDFæ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”±å¤§æ¨¡å‹ä¼˜åŒ–è¾“å‡ºï¼ˆæµå¼å“åº”ï¼‰")

            with gr.Row():
                user_query = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šä¸Šæµ·æœ‰å“ªäº›æ¨èæ™¯ç‚¹ï¼Ÿ")
                ask_btn = gr.Button("æ£€ç´¢æ–‡æ¡£", variant="primary")

            with gr.Row():
                rag_answer = gr.Textbox(label="æ£€ç´¢ç»“æœ", lines=15, interactive=False)

            def query_docs_with_rag_stream(query):
                if not query.strip():
                    yield "è¯·è¾“å…¥é—®é¢˜"
                    return
                yield from stream_search_docs(query, retriever)

            ask_btn.click(fn=query_docs_with_rag_stream, inputs=[user_query], outputs=[rag_answer], stream=True)

        demo.launch()
