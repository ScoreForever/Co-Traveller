from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from pathlib import Path
import fitz
import os
import gradio as gr
import requests
import sseclient
import json

# è¿œç¨‹ Qwen API æµå¼å“åº”å°è£…
# def stream_qwen_response(prompt):
#     api_key = os.getenv("sub_SILICON_API_KEY")
#     if not api_key:
#         raise ValueError("è¯·è®¾ç½® sub_SILICON_API_KEY ç¯å¢ƒå˜é‡")

#     headers = {
#         "Authorization": f"Bearer {api_key}",
#         "Accept": "text/event-stream",
#         "Content-Type": "application/json"
#     }
#     payload = {
#         "model": "Qwen/Qwen3-8B",
#         "stream": True,
#         "messages": [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡åŠ©æ‰‹ã€‚"},
#             {"role": "user", "content": prompt}
#         ]
#     }
def stream_qwen_response(prompt):
    import os
    import requests
    import json

    api_key = os.getenv("SILICON_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® SILICON_API_KEY ç¯å¢ƒå˜é‡")

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Accept": "text/event-stream",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "Qwen/Qwen3-8B",
        "stream": True,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    }

    with requests.post("https://api.siliconflow.cn/v1/chat/completions", headers=headers, json=payload, stream=True) as response:
        if response.status_code != 200:
            raise RuntimeError(f"è¯·æ±‚å¤±è´¥ï¼š{response.status_code} {response.text}")
        response.encoding = "utf-8"
        for line in response.iter_lines(decode_unicode=True):
            if not line or not line.startswith("data: "):
                continue

            data = line.removeprefix("data: ").strip()
            if data == "[DONE]":
                break

            try:
                delta = json.loads(data)
                content = delta.get("choices", [{}])[0].get("delta", {}).get("content", "")
                if content:
                    yield content
            except json.JSONDecodeError as e:
                # ä¸å†è¾“å‡ºæŠ¥é”™ï¼Œé¿å…æ±¡æŸ“ UI æ˜¾ç¤º
                continue

    with requests.post("https://api.siliconflow.cn/v1/chat/completions", headers=headers, json=payload, stream=True) as response:
        if response.status_code != 200:
            raise RuntimeError(f"è¯·æ±‚å¤±è´¥ï¼š{response.status_code} {response.text}")

        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼è¿”å›
        if "text/event-stream" not in response.headers.get("Content-Type", ""):
            raise RuntimeError(f"ä¸æ˜¯ SSE æµå¼è¿”å›ï¼Œè€Œæ˜¯ï¼š{response.headers.get('Content-Type')}")
        client = sseclient.SSEClient(response)
        for event in client.events():
            if event.data == "[DONE]":
                break
            try:
                delta = json.loads(event.data)
                content = delta.get("choices", [{}])[0].get("delta", {}).get("content", "")
                yield content
            except Exception as e:
                yield f"[æµå¼è§£æé”™è¯¯] {e}"

# åŠ è½½ PDF æ–‡ä»¶å¤¹
def load_pdfs_from_folder(folder_path):
    documents = []
    for file in Path(folder_path).rglob("*.pdf"):
        try:
            text = ""
            with fitz.open(str(file)) as doc:
                for page in doc:
                    text += page.get_text()
            if text.strip():
                documents.append(Document(page_content=text, metadata={"source": str(file)}))
        except Exception as e:
            print(f"è¯»å–å¤±è´¥: {file} åŸå› : {e}")
    return documents

# æ„å»ºä»…æ£€ç´¢çš„å‘é‡æ£€ç´¢å™¨
def build_retriever_from_docs(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)
    if not chunks:
        raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“")

    # embedder = HuggingFaceEmbeddings(
    #     model_name="./models/bge-small-zh",
    #     model_kwargs={"device": "cpu"}
    # )

    # ä»¥å½“å‰ travel.py æ‰€åœ¨çš„ src ç›®å½•ä¸ºåŸºå‡†
    src_dir = Path(__file__).resolve().parent.parent
    model_path = src_dir / "models" / "bge-small-zh"

    # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedder = HuggingFaceEmbeddings(
        model_name=str(model_path),
        model_kwargs={"device": "cpu"}
    )
    


    vectordb = FAISS.from_documents(chunks, embedder)
    return vectordb.as_retriever(search_kwargs={"k": 10})

# æ„å»ºæœç´¢å‡½æ•°ï¼ˆå¯¹æ£€ç´¢ç»“æœè¿›è¡Œæµå¼æ€»ç»“ï¼‰
def stream_search_docs(query, retriever):
    results = retriever.invoke(query)  # ä½¿ç”¨æ–°ç‰ˆ API
    if not results:
        yield "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        return

    filtered = [doc for doc in results if query.lower() in doc.page_content.lower()]
    if not filtered:
        yield "æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„å†…å®¹ï¼Œè¯·å°è¯•æ¢ä¸ªè¡¨è¾¾"
        return

    combined_text = "\n".join(doc.page_content[:1000] for doc in filtered[:5])
    try:
        prompt = f"è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆç®€æ´æ¸…æ™°çš„æ—…æ¸¸æ¨èæ‘˜è¦ï¼š\n\n{combined_text}\n\næ‘˜è¦ï¼š"
        for chunk in stream_qwen_response(prompt):
            yield chunk
    except Exception as e:
        #yield f"å¤§æ¨¡å‹æ€»ç»“å¤±è´¥ï¼š{e}ï¼Œæ”¹ä¸ºæ˜¾ç¤ºåŸæ–‡ï¼š\n\n{combined_text}"
        return

# åŠ è½½ç¯å¢ƒå˜é‡
def load_env(filepath):
    env = {}
    if os.path.exists(filepath):
        with open(filepath, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" in line:
                    k, v = line.split("=", 1)
                    env[k.strip()] = v.strip()
    return env

if __name__ == "__main__":
    env_path = Path(__file__).resolve().parent.parent / "API.env"
    env_vars = load_env(env_path)
    os.environ.update(env_vars)

    dataset_dir = Path(__file__).resolve().parent.parent / "dataset"
    #dataset_dir = Path("./dataset").resolve()
    #dataset_dir = Path("dataset").resolve()
    rag_docs = load_pdfs_from_folder(dataset_dir)
    if not rag_docs:
        raise RuntimeError("PDF æ–‡ä»¶å¤¹ä¸­æœªæˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ dataset è·¯å¾„ä¸ PDF å†…å®¹")
    retriever = build_retriever_from_docs(rag_docs)

    # dataset_dir = Path(__file__).resolve().parent.parent / "dataset"
    # # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜
    # print(f"ğŸ“‚ æ­£åœ¨åŠ è½½ dataset æ–‡ä»¶å¤¹è·¯å¾„: {dataset_dir}")
    # if not dataset_dir.exists():
    #     raise FileNotFoundError(f"æ‰¾ä¸åˆ° dataset æ–‡ä»¶å¤¹ï¼Œè¯·ç¡®è®¤è·¯å¾„æ˜¯å¦å­˜åœ¨: {dataset_dir}")

    # åŠ è½½ PDF æ–‡æ¡£
    rag_docs = load_pdfs_from_folder(dataset_dir)
    if not rag_docs:
        raise RuntimeError("âŒ PDF æ–‡ä»¶å¤¹ä¸­æœªæˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ dataset è·¯å¾„ä¸ PDF å†…å®¹")

    # æ„å»ºæ£€ç´¢å™¨
    retriever = build_retriever_from_docs(rag_docs)

    with gr.Blocks() as demo:
        with gr.Tab("ğŸ“š æ–‡æ¡£é—®ç­”åŠ©æ‰‹"):
            gr.Markdown("### è¾“å…¥å…³é”®è¯ï¼ˆå¦‚åŸå¸‚åï¼‰ï¼Œä»PDFæ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”±å¤§æ¨¡å‹ä¼˜åŒ–è¾“å‡ºï¼ˆæµå¼å“åº”ï¼‰")

            with gr.Row():
                user_query = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šåŒ—äº¬")
                ask_btn = gr.Button("æ£€ç´¢æ–‡æ¡£", variant="primary")

            with gr.Row():
                rag_answer = gr.Textbox(label="æ£€ç´¢ç»“æœ", lines=15, interactive=False)

            def query_docs_with_rag_stream(query):
                if not query.strip():
                    yield "è¯·è¾“å…¥é—®é¢˜"
                    return
                yield from stream_search_docs(query, retriever)

            ask_btn.click(fn=query_docs_with_rag_stream, inputs=[user_query], outputs=[rag_answer], stream=True)

        demo.launch()
